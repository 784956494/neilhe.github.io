---
title: "Position: Beyond Euclidean -- Foundation Models Should Embrace Non-Euclidean Geometries"
collection: publications
category: preprints
permalink: /publication/2025-4-position
excerpt: 'This paper argues for the necessity of incoporating non-Euclidean geometry into foundation models design, with arguments grounded in both theoretics and pratical considerations'
date: 2025-04-11
venue: 'preprint, under review'
paperurl: 'https://arxiv.org/abs/2504.08896'
citation: 'Neil He, Jiahong Liu, Buze Zhang, Ngoc Bui, Ali Maatouk, Menglin Yang, Irwin King, Melanie Weber, and Rex Ying. &quot;Position: Beyond Euclidean -- Foundation Models Should Embrace Non-Euclidean Geometries.&quot; <i>arXiv preprint</i>. 2025.'
---

In the era of foundation models and Large Language Models (LLMs), Euclidean space has been the de facto geometric setting for machine learning architectures. However, recent literature has demonstrated that this choice comes with fundamental limitations. At a large scale, real-world data often exhibit inherently non-Euclidean structures, such as multi-way relationships, hierarchies, symmetries, and non-isotropic scaling, in a variety of domains, such as languages, vision, and the natural sciences. It is challenging to effectively capture these structures within the constraints of Euclidean spaces. This position paper argues that moving beyond Euclidean geometry is not merely an optional enhancement but a necessity to maintain the scaling law for the next-generation of foundation models. By adopting these geometries, foundation models could more efficiently leverage the aforementioned structures. Task-aware adaptability that dynamically reconfigures embeddings to match the geometry of downstream applications could further enhance efficiency and expressivity. Our position is supported by a series of theoretical and empirical investigations of prevalent foundation models. Finally, we outline a roadmap for integrating non-Euclidean geometries into foundation models, including strategies for building geometric foundation models via fine-tuning, training from scratch, and hybrid approaches.

